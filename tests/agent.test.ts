import { Node, run, Action } from '../src';
import { mockLLM } from './mock-llm';

/**
 * # Tutorial: Building a Question-Answering Agent with Iterative Search
 *
 * > **[View example code](../../tests/agent.test.ts)**
 *
 * ## What Will Be Built
 *
 * A question-answering agent that iteratively searches for information until it has
 * sufficient context to provide an answer. The agent will evaluate its current knowledge,
 * perform web searches to gather more context, and decide when it's ready to generate
 * a final answer.
 *
 * Input:  { question: 'What is the latest news about AI?', context: [], iterations: 0 }
 * Output: { question: '...', context: ['Search 1', 'Search 2'], answer: '...', iterations: 2 }
 *
 * ## Workflow Diagram
 *
 * ```mermaid
 * graph LR
 *     Store1["Store (initial)
 *     ―――――――――――――
 *     question: '...'
 *     context: []
 *     iterations: 0"]
 *
 *     Decision{"DecisionNode
 *     ―――――――――――――
 *     Evaluate context
 *     Decide action"}
 *
 *     Search["SearchNode
 *     ―――――――――――――
 *     Perform search
 *     Add to context
 *     iterations++"]
 *
 *     Answer["AnswerNode
 *     ―――――――――――――
 *     Generate answer
 *     from context"]
 *
 *     Store2["Store (final)
 *     ―――――――――――――
 *     context: [...]
 *     answer: '...'
 *     iterations: 2"]
 *
 *     Store1 --> Decision
 *     Decision -->|"action: 'search'
 *     (insufficient context)"| Search
 *     Decision -->|"action: 'answer'
 *     (sufficient context)"| Answer
 *     Search -->|"default edge"| Decision
 *     Answer --> Store2
 * ```
 *
 * ## Implementation
 *
 * The workflow consists of three nodes with dynamic routing:
 *
 * **DecisionNode**: The current question and accumulated context will be evaluated by
 * `exec()`. A decision will be made to either search for more information or generate
 * an answer. The action ('search' or 'answer') will be returned by `post()`, routing
 * to the appropriate node.
 *
 * **SearchNode**: A web search will be performed by `exec()` using the question. The
 * search results will be appended to the context array by `post()`. The default edge
 * will return control to DecisionNode for re-evaluation.
 *
 * **AnswerNode**: The final answer will be generated by `exec()` using all accumulated
 * context. The answer will be stored by `post()`, and the workflow will terminate.
 *
 * **Loop Control**: The agent will continue looping between DecisionNode and SearchNode
 * until DecisionNode determines sufficient context has been gathered, at which point it
 * will route to AnswerNode.
 *
 * @example
 * const store: AgentStore = {
 *   question: 'What is the latest news about AI?',
 *   context: [],
 *   iterations: 0
 * };
 *
 * const decisionNode = new DecisionNode();
 * const searchNode = new SearchNode();
 * const answerNode = new AnswerNode();
 *
 * // Connect decision node to both search and answer paths
 * decisionNode
 *   .connect('search', searchNode)
 *   .connect('answer', answerNode);
 *
 * // Search node loops back to decision node
 * searchNode.connect(decisionNode);
 *
 * // Agent will iteratively search until ready to answer
 * await run(decisionNode, store);
 *
 * // Final state after agent completes
 * console.log(store.iterations); // 2
 * console.log(store.context);    // ['Search results 1', 'Search results 2']
 * console.log(store.answer);     // 'Final answer based on context'
 */

interface AgentStore {
  question: string;
  context: string[];
  answer?: string;
  iterations: number;
}

interface AgentDecision {
  action: 'search' | 'answer';
  reasoning: string;
  query?: string;
}

class DecisionNode extends Node<AgentStore, string, AgentDecision> {
  async *prep(store: AgentStore) {
    const contextStr = store.context.join('\n');
    yield `Question: ${store.question}
Context so far: ${contextStr}
Decide whether to search for more information or answer the question.`;
  }

  async exec(store: AgentStore, prompt: string): Promise<AgentDecision> {
    await mockLLM.call(prompt);

    if (store.context.length < 2) {
      return {
        action: 'search',
        reasoning: 'Need more information',
        query: store.question
      };
    } else {
      return {
        action: 'answer',
        reasoning: 'Have sufficient context'
      };
    }
  }

  async post(
    store: AgentStore,
    prepItems: string[],
    execResults: AgentDecision[]
  ): Promise<Action> {
    const decision = execResults[0];
    return decision.action;
  }
}

class SearchNode extends Node<AgentStore, string, string> {
  async *prep(store: AgentStore) {
    yield store.question;
  }

  async exec(store: AgentStore, query: string): Promise<string> {
    return mockLLM.searchWeb(query);
  }

  async post(
    store: AgentStore,
    prepItems: string[],
    execResults: string[]
  ): Promise<Action> {
    store.context.push(execResults[0]);
    store.iterations++;
    return 'default';
  }
}

class AnswerNode extends Node<AgentStore, string, string> {
  async *prep(store: AgentStore) {
    const contextStr = store.context.join('\n');
    yield `Question: ${store.question}\nContext: ${contextStr}\nAnswer:`;
  }

  async exec(store: AgentStore, prompt: string): Promise<string> {
    return mockLLM.call(prompt);
  }

  async post(
    store: AgentStore,
    prepItems: string[],
    execResults: string[]
  ): Promise<Action> {
    store.answer = execResults[0];
    return null;
  }
}

describe('Agent Pattern', () => {
  test('agent makes decisions and loops until ready to answer', async () => {
    const store: AgentStore = {
      question: 'What is the latest news about AI?',
      context: [],
      iterations: 0
    };

    const decisionNode = new DecisionNode();
    const searchNode = new SearchNode();
    const answerNode = new AnswerNode();

    decisionNode
      .connect('search', searchNode)
      .connect('answer', answerNode);

    searchNode.connect(decisionNode);

    await run(decisionNode, store);

    expect(store.iterations).toBeGreaterThan(0);
    expect(store.context.length).toBeGreaterThan(0);
    expect(store.answer).toBeDefined();
  });
});
